import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.ensemble import (
    GradientBoostingRegressor, RandomForestRegressor, StackingRegressor,
    GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier,
    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
)
from sklearn.linear_model import Ridge, LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neural_network import MLPRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import StackingRegressor
from statsmodels.tsa.arima.model import ARIMA
from stable_baselines3 import PPO
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import matplotlib
matplotlib.use('Agg')  # â† â˜… ã“ã®è¡Œã‚’å…ˆã«è¿½åŠ ï¼
import matplotlib.pyplot as plt
import aiohttp
from random import shuffle
import asyncio
import warnings
import re
import platform
import gym
import sys
import os
import random
from sklearn.metrics import precision_score, recall_score, f1_score
from neuralforecast.models import TFT
from neuralforecast import NeuralForecast
import onnxruntime
import streamlit as st
from autogluon.tabular import TabularPredictor
import torch.backends.cudnn
from datetime import datetime 
from collections import Counter
import torch.nn.functional as F
import math

# Windowsç’°å¢ƒã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒãƒªã‚·ãƒ¼ã‚’è¨­å®š
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

warnings.filterwarnings("ignore")

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

def set_global_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_global_seed()

import subprocess

def git_commit_and_push(file_path, message):
    try:
        subprocess.run(["git", "add", file_path], check=True)
        diff = subprocess.run(["git", "diff", "--cached", "--quiet"])
        if diff.returncode != 0:
            subprocess.run(["git", "config", "--global", "user.name", "github-actions"], check=True)
            subprocess.run(["git", "config", "--global", "user.email", "github-actions@github.com"], check=True)
            subprocess.run(["git", "commit", "-m", message], check=True)
            subprocess.run(["git", "push"], check=True)
        else:
            print(f"[INFO] No changes in {file_path}")
    except Exception as e:
        print(f"[WARNING] Git commit/push failed: {e}")

def calculate_reward(selected_numbers, winning_numbers, cycle_scores):
    match_count = len(set(selected_numbers) & set(winning_numbers))
    avg_cycle_score = np.mean([cycle_scores.get(n, 999) for n in selected_numbers])
    reward = match_count * 0.5 + max(0, 1 - avg_cycle_score / 50)
    return reward

class LotoEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(LotoEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)

    def reset(self):
        return np.zeros(10, dtype=np.float32)

def step(self, action):
    if action.size == 0:
        return np.zeros(10, dtype=np.float32), -1.0, True, {}

    selected_numbers = set(np.argsort(action)[-3:])
    target_numbers = set(self.target_numbers_list[self.current_index])

    match_count = len(selected_numbers & target_numbers)
    # cycle_scores ã‚’ self.cycle_scores ã§æŒã£ã¦ã„ãªã„å ´åˆã¯ã€é©å½“ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã™ã‚‹
    avg_cycle_score = 999  # ä»®ã«å›ºå®šå€¤ã‚’è¨­å®š
    reward = match_count * 0.5 + max(0, 1 - avg_cycle_score / 50)

    done = True
    obs = np.zeros(10, dtype=np.float32)
    return obs, reward, done, {}

class DiversityEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(DiversityEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.previous_outputs = set()

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        if action.size == 0:
            return np.zeros(10, dtype=np.float32), -1.0, True, {}  # ã‚¨ãƒ©ãƒ¼å›é¿

        selected = np.argsort(action)[-3:]  # ã¾ãŸã¯[-4:]

        selected = tuple(sorted(np.argsort(action)[-4:]))
        reward = 1.0 if selected not in self.previous_outputs else -1.0
        self.previous_outputs.add(selected)
        return np.zeros(10, dtype=np.float32), reward, True, {}

class CycleEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(CycleEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.cycle_scores = calculate_number_cycle_score(historical_numbers)

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        if action.size == 0:
            return np.zeros(10, dtype=np.float32), -1.0, True, {}  # ã‚¨ãƒ©ãƒ¼å›é¿

        selected = np.argsort(action)[-3:]  # ã¾ãŸã¯[-4:]

        selected = np.argsort(action)[-4:]
        avg_cycle = np.mean([self.cycle_scores.get(n, 999) for n in selected])
        reward = max(0, 1 - (avg_cycle / 50))
        return np.zeros(10, dtype=np.float32), reward, True, {}

class ProfitLotoEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(ProfitLotoEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)

    def reset(self):
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        if action.size == 0:
            return np.zeros(10, dtype=np.float32), -1.0, True, {}  # ã‚¨ãƒ©ãƒ¼å›é¿

        selected = np.argsort(action)[-3:]  # ã¾ãŸã¯[-4:]

        selected = list(np.argsort(action)[-3:])
        reward_table = {
            "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ": 90000,
            "ãƒœãƒƒã‚¯ã‚¹": 10000,
            "ãƒŸãƒ‹": 4000,
            "ã¯ãšã‚Œ": -200
        }
        best_reward = -200
        for winning in self.historical_numbers:
            result = classify_numbers3_prize(selected, winning)
            reward = reward_table.get(result, -200)
            if reward > best_reward:
                best_reward = reward
        return np.zeros(10, dtype=np.float32), best_reward, True, {}

class MultiAgentPPOTrainer:
    def __init__(self, historical_data, total_timesteps=5000):
        self.historical_data = historical_data
        self.total_timesteps = total_timesteps
        self.agents = {}

    def train_agents(self):
        envs = {
            "accuracy": LotoEnv(self.historical_data),
            "diversity": DiversityEnv(self.historical_data),
            "cycle": CycleEnv(self.historical_data),
            "profit": ProfitLotoEnv(self.historical_data)  # â˜… ã“ã“ã‚’è¿½åŠ 
        }

        for name, env in envs.items():
            model = PPO("MlpPolicy", env, verbose=0)
            model.learn(total_timesteps=self.total_timesteps)
            self.agents[name] = model
            print(f"[INFO] PPO {name} ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’å®Œäº†")

    def predict_all(self, num_candidates=50):
        predictions = []
        for name, model in self.agents.items():
            obs = model.env.reset()
            for _ in range(num_candidates // 3):
                action, _ = model.predict(obs)
                selected = list(np.argsort(action)[-4:])
                predictions.append((selected, 0.9))  # ä¿¡é ¼åº¦ã¯ä»®
        return predictions

class AdversarialLotoEnv(gym.Env):
    def __init__(self, target_numbers_list):
        """
        GANãŒç”Ÿæˆã—ãŸç•ªå·ï¼ˆtarget_numbers_listï¼‰ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã€
        PPOã«ã€Œãã‚Œã‚‰ã‚’å½“ã¦ã•ã›ã‚‹ã€å¯¾æˆ¦ç’°å¢ƒ
        """
        super(AdversarialLotoEnv, self).__init__()
        self.target_numbers_list = target_numbers_list
        self.current_index = 0
        self.action_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)

    def reset(self):
        self.current_index = (self.current_index + 1) % len(self.target_numbers_list)
        return np.zeros(10, dtype=np.float32)

    def step(self, action):
        if action.size == 0:
            return np.zeros(10, dtype=np.float32), -1.0, True, {}

        selected_numbers = set(np.argsort(action)[-3:])
        target_numbers = set(self.target_numbers_list[self.current_index])

        match_count = len(selected_numbers & target_numbers)
        avg_cycle_score = np.mean([self.cycle_scores.get(n, 999) for n in selected_numbers])
        reward = match_count * 0.5 + max(0, 1 - avg_cycle_score / 50)

        done = True
        obs = np.zeros(10, dtype=np.float32)
        return obs, reward, done, {}

def score_real_structure_similarity(numbers):
    """
    æ•°å­—ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€ã€Œæœ¬ç‰©ã‚‰ã—ã„æ§‹é€ ã‹ã©ã†ã‹ã€ã‚’è©•ä¾¡ã™ã‚‹ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ1ï¼‰
    - åˆè¨ˆãŒ10ã€œ20
    - é‡è¤‡ãŒãªã„
    - ä¸¦ã³ãŒæ˜‡é † or é™é †
    """
    if len(numbers) != 3:
        return 0
    score = 0
    if 10 <= sum(numbers) <= 20:
        score += 1
    if len(set(numbers)) == 3:
        score += 1
    if numbers == sorted(numbers) or numbers == sorted(numbers, reverse=True):
        score += 1
    return score / 3  # æœ€å¤§3ç‚¹æº€ç‚¹ã‚’0ã€œ1ã‚¹ã‚±ãƒ¼ãƒ«

class LotoGAN(nn.Module):
    def __init__(self, noise_dim=100):
        super(LotoGAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10),
            nn.Sigmoid()
        )
        self.discriminator = nn.Sequential(
            nn.Linear(10, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.noise_dim = noise_dim

    def generate_samples(self, num_samples):
        noise = torch.randn(num_samples, self.noise_dim)
        with torch.no_grad():
            samples = self.generator(noise)
        return samples.numpy()

    def evaluate_generated_numbers(self, sample_tensor):
        """
        sample_tensor: shape=(10,) ã®Tensorï¼ˆ0ã€œ1å€¤ã§å„æ•°å­—ã®ã‚¹ã‚³ã‚¢ï¼‰
        ä¸Šä½3ã¤ã‚’é¸ã‚“ã§ç•ªå·ã«å¤‰æ› â†’ åˆ¤åˆ¥å™¨ã‚¹ã‚³ã‚¢ã¨æ§‹é€ ã‚¹ã‚³ã‚¢ã‚’åˆæˆ
        """
        numbers = list(np.argsort(sample_tensor.cpu().numpy())[-3:])
        numbers.sort()
        real_score = score_real_structure_similarity(numbers)

        with torch.no_grad():
            discriminator_score = self.discriminator(sample_tensor.unsqueeze(0)).item()

        final_score = 0.5 * discriminator_score + 0.5 * real_score
        return final_score

class DiffusionNumberGenerator(nn.Module):
    def __init__(self, noise_dim=16, steps=100):
        super(DiffusionNumberGenerator, self).__init__()
        self.noise_dim = noise_dim
        self.steps = steps

        self.model = nn.Sequential(
            nn.Linear(noise_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 10),  # å„æ•°å­—ã®ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ9ï¼‰
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

    def generate(self, num_samples=20):
        samples = []
        for _ in range(num_samples):
            noise = torch.randn(1, self.noise_dim)
            x = noise
            for _ in range(self.steps):
                noise_grad = torch.randn_like(x) * 0.1
                x = x - 0.01 * x + noise_grad
            with torch.no_grad():
                scores = self.forward(x).squeeze().numpy()
            top4 = np.argsort(scores)[-4:]
            samples.append(sorted(top4.tolist()))
        return samples

def create_advanced_features(dataframe):
    dataframe = dataframe.copy()
    def convert_to_number_list(x):
        if isinstance(x, str):
            cleaned = x.strip("[]").replace(",", " ").replace("'", "").replace('"', "")
            return [int(n) for n in cleaned.split() if n.isdigit()]
        return x if isinstance(x, list) else [0]

    dataframe['æœ¬æ•°å­—'] = dataframe['æœ¬æ•°å­—'].apply(convert_to_number_list)
    dataframe['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(dataframe['æŠ½ã›ã‚“æ—¥'])

    valid_mask = (dataframe['æœ¬æ•°å­—'].apply(len) == 3)
    dataframe = dataframe[valid_mask].copy()

    if dataframe.empty:
        print("[ERROR] æœ‰åŠ¹ãªæœ¬æ•°å­—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆ4æ¡ãƒ‡ãƒ¼ã‚¿ãŒãªã„ï¼‰")
        return pd.DataFrame()  # ç©ºã®DataFrameã‚’è¿”ã™

    nums_array = np.vstack(dataframe['æœ¬æ•°å­—'].values)
    features = pd.DataFrame(index=dataframe.index)

    features['æ•°å­—åˆè¨ˆ'] = nums_array.sum(axis=1)
    features['æ•°å­—å¹³å‡'] = nums_array.mean(axis=1)
    features['æœ€å¤§'] = nums_array.max(axis=1)
    features['æœ€å°'] = nums_array.min(axis=1)
    features['æ¨™æº–åå·®'] = np.std(nums_array, axis=1)

    return pd.concat([dataframe, features], axis=1)

def preprocess_data(data):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†: ç‰¹å¾´é‡ã®ä½œæˆ & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
    
    # ç‰¹å¾´é‡ä½œæˆ
    processed_data = create_advanced_features(data)

    if processed_data.empty:
        print("ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ç”Ÿæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None, None, None

    print("=== ç‰¹å¾´é‡ä½œæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(processed_data.head())

    # æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
    numeric_features = processed_data.select_dtypes(include=[np.number]).columns
    X = processed_data[numeric_features].fillna(0)  # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹

    print(f"æ•°å€¤ç‰¹å¾´é‡ã®æ•°: {len(numeric_features)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

    if X.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ•°å€¤ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œãšã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã£ã¦ã„ã¾ã™ã€‚")
        return None, None, None

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    print("=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(X_scaled[:5])  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º

    # ç›®æ¨™å¤‰æ•°ã®æº–å‚™
    try:
        y = np.array([list(map(int, nums)) for nums in processed_data['æœ¬æ•°å­—']])
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ç›®æ¨™å¤‰æ•°ã®ä½œæˆæ™‚ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None, None

    return X_scaled, y, scaler

def convert_numbers_to_binary_vectors(data):
    vectors = []
    for numbers in data['æœ¬æ•°å­—']:
        vec = np.zeros(10)
        for n in numbers:
            if 0 <= n <= 9:
                vec[n] = 1
        vectors.append(vec)
    return np.array(vectors)

def calculate_prediction_errors(predictions, actual_numbers):
    """äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å½“é¸çµæœã®èª¤å·®ã‚’è¨ˆç®—ã—ã€ç‰¹å¾´é‡ã¨ã—ã¦ä¿å­˜"""
    errors = []
    for pred, actual in zip(predictions, actual_numbers):
        pred_numbers = set(pred[0])
        actual_numbers = set(actual)
        error_count = len(actual_numbers - pred_numbers)
        errors.append(error_count)
    
    return np.mean(errors)

def enforce_grade_structure(predictions, min_required=3):
    """ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆãƒ»ãƒœãƒƒã‚¯ã‚¹ãƒ»ãƒŸãƒ‹æ§‹æˆã‚’å¿…ãšå«ã‚ã‚‹ (originå¯¾å¿œç‰ˆ)"""
    from itertools import permutations

    forced = []
    used = set()

    # ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆæ§‹æˆï¼ˆãã®ã¾ã¾ï¼‰
    for pred in predictions:
        if len(pred) == 3:
            numbers, conf, origin = pred
        else:
            numbers, conf = pred
            origin = "Unknown"

        t = tuple(numbers)
        if t not in used:
            used.add(t)
            forced.append((t, conf, origin))
            if len(forced) >= 1:
                break

    # ãƒœãƒƒã‚¯ã‚¹æ§‹æˆï¼ˆä¸¦ã³æ›¿ãˆï¼‰
    for pred in predictions:
        if len(pred) == 3:
            numbers, conf, origin = pred
        else:
            numbers, conf = pred
            origin = "Unknown"

        for perm in permutations(numbers):
            if perm not in used:
                used.add(perm)
                forced.append((perm, conf, origin))
                break
        if len(forced) >= 2:
            break

    # ãƒŸãƒ‹æ§‹æˆï¼ˆ2æ•°å­—ä¸€è‡´ï¼‰
    for pred in predictions:
        if len(pred) == 3:
            numbers, conf, origin = pred
        else:
            numbers, conf = pred
            origin = "Unknown"

        for known in used:
            if len(set(numbers) & set(known)) == 2:
                t = tuple(numbers)
                if t not in used:
                    used.add(t)
                    forced.append((t, conf, origin))
                    break
        if len(forced) >= min_required:
            break

    return forced + predictions

def delete_old_generation_files(directory, days=1):
    """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã§ã€æŒ‡å®šæ—¥æ•°ã‚ˆã‚Šå¤ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    now = datetime.now()
    deleted = 0
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath) and filename.endswith(".csv"):
            try:
                modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))
                if (now - modified_time).days >= days:
                    os.remove(filepath)
                    deleted += 1
            except Exception as e:
                print(f"[WARNING] ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {filename} â†’ {e}")
    if deleted:
        print(f"[INFO] {deleted} ä»¶ã®å¤ã„ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

def save_self_predictions(predictions, file_path="self_predictions.csv", max_records=100, historical_data=None):
    """äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜ã—ã€ä¸€è‡´æ•°ã¨ç­‰ç´šã‚‚è¨˜éŒ²"""
    rows = []
    valid_grades = ["ã¯ãšã‚Œ", "ãƒœãƒƒã‚¯ã‚¹", "ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ"]  
    for numbers, confidence in predictions:
        match_count = "-"
        prize = "-"
        if historical_data is not None:
            actual_list = [parse_number_string(x) for x in historical_data['æœ¬æ•°å­—'].tolist()]
            match_count = max(len(set(numbers) & set(actual)) for actual in actual_list)
            prize = max(
                (classify_numbers3_prize(numbers, actual) for actual in actual_list),
                key=lambda p: valid_grades.index(p) if p in valid_grades else -1
            )
        rows.append(numbers + [confidence, match_count, prize])

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ä¸­èº«ãŒç©ºã§ãªã„å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            existing = pd.read_csv(file_path, header=None).values.tolist()
            rows = existing + rows
        except pd.errors.EmptyDataError:
            print(f"[WARNING] {file_path} ã¯ç©ºã®ãŸã‚ã€èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        print(f"[INFO] {file_path} ãŒç©ºã‹å­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")

    # æœ€æ–° max_records ä»¶ã«åˆ¶é™ã—ã¦ä¿å­˜
    rows = rows[-max_records:]
    df = pd.DataFrame(rows)
    df.to_csv(file_path, index=False, header=False)
    print(f"[INFO] è‡ªå·±äºˆæ¸¬ã‚’ {file_path} ã«ä¿å­˜ï¼ˆæœ€å¤§{max_records}ä»¶ï¼‰")

    # ğŸ” ä¸–ä»£åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    gen_dir = "self_predictions_gen"
    os.makedirs(gen_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generation_file = os.path.join(gen_dir, f"self_predictions_gen_{timestamp}.csv")
    df.to_csv(generation_file, index=False, header=False)
    print(f"[INFO] ä¸–ä»£åˆ¥äºˆæ¸¬ã‚’ä¿å­˜: {generation_file}")

    # ğŸ§¹ å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‰Šé™¤ï¼ˆ1æ—¥ä»¥ä¸Šå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼‰
    delete_old_generation_files(gen_dir, days=1)

def load_self_predictions(
    file_path="self_predictions.csv",
    min_match_threshold=3,
    true_data=None,
    min_grade="ãƒœãƒƒã‚¯ã‚¹",
    return_with_freq=True,
    max_date=None  # â† â˜… è¿½åŠ 
):
    if not os.path.exists(file_path):
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    if os.path.getsize(file_path) == 0:
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã¯ç©ºã§ã™ã€‚")
        return None

    try:
        df = pd.read_csv(file_pat
